# Exsercise 2

### The data reading and exploring
```{r}
students14 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt", sep=",", header=TRUE)
head(students14)
str(students14)
dim(students14)
```
The data is collected from the survey on teaching and learning. In this data, we have 166 students' answers. In each observation, we have a responder's age and gender. Also, we have 4 variables that measure student's deep, surface and strategic approach to learning and his/her attitude toward statistics. The last variable "points" tells us his/her exam score. 

```{r message = FALSE}
library(ggplot2)
library(GGally)

summary(students14)
ggpairs(students14, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(students14, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
```
Let us first describe the distributions of the variables. Then we will take a look at their connection to each other, especially to the "points" variable. 

The "gender" variable is nominal. It means that it can take only 2 values, 0 and 1. 0 stands for woman and 1 - for man. According to the plot, we have more women answered this survey than men. 
Also, we notice that on average, our responders are quite young. 

With this data, it is very hard to notice any corrections between this variable. Fortunately, the plot helps us. The strongest correlation happens between "attitude" and "points" variables. Additionally, there is a negative correlation (-0.3, not very strong) between deep and surface approaches. 

Structural and surface approaches correlate with points, as well. I think that those variables and "attitude" should be chosen as explanatory variables for a regression model where exam points are the response variable. 

### Choosing the right model

Let us start with this model:
  y_i = b_0 + b_1\*x_(attitude, i) + b_2\*x_(stra, i) + b_3\*x(surf, i)  
We are going to fit it now.

```{r}
model <- lm(points ~ attitude + stra + surf, data = students14)
summary(model)
```

So we get the following results:
  - Regression coefficients are 3.4 for "attitude", 0.9 for "stra" and -0.6 for "surf". 
  - However, the p-values for "stra" and "surf" are very big (>0.1). It means that those regression coefficients may be zero. So these 2 variables are not statistically significant with the target variable.
  - The P-value for "attitude" is almost zero, which means this variable has a statistically significant relationship with the "point" variable.
  
Let us remove statistically no-significant variables and fit the model again.

```{r}
model <- lm(points ~ attitude, data = students14)
summary(model)
```

In this model, we have only the "attitude" variable. The regression coefficient is 3.5. It means that when a student's attitude towards statistics grows by one point, a student gets 3.5 points more.
The multiple R-squared of this model is 0.19, which means that the "attitude" variable explains 20% of the variation in the "points" variable.

### Model diagnostics

When we use this model, we make 2 assumptions:
1. The variable "points" is normally distributed
2. e_i is normally distributed with the mean of 0 and constant variance.

The plots we are going to plot now will help us prove this assumption.

```{r}
par(mfrow = c(2,2))
plot(model, which = c(1,2,5))
```

Normal Q-Q shows that there is little evidence of a departure from linearity. That means that there is no evidence that the "points" variable is not normally distributed.

"Residuals vs Fitted" plot does not show that there is the variance of the residuals increases on decreases with the fitted values. So we can say that the variance of e_i is constant. 

Leverage measures how much impact a single observation has on a model. One of observation's leverage is 0.04. It is very small. So we can say that this data does not have any outliers that would affect the model.

